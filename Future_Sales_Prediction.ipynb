{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Sales Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This script uses a time-series dataset consisting of daily sales of all \n",
    "products for a software company with multiple stores/branches (train set), \n",
    "evaluates several models on the train set, and uses the best model to predict \n",
    "the total sales for every product and store in the following month (test set).\n",
    "\n",
    "'''\n",
    "__author__ = \"Mahsa Shokouhi\"\n",
    "__email__ = \"mahsa_shokouhi@yahoo.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(df):\n",
    "    ''' Overview of a dataframe '''\n",
    "    print('Dimensions:', df.shape)\n",
    "    print('\\nThe first and last 3 lines:')\n",
    "\n",
    "    return df.iloc[list(range(3))+list(range(-3, 0))]\n",
    "\n",
    "\n",
    "def add_date_features(df):\n",
    "    ''' Extract and add year, month, and season to features. '''\n",
    "    df['year'] = df.date.map(lambda x: x.year)\n",
    "    df['month'] = df.date.map(lambda x: x.month)\n",
    "    df['season'] = ((df.month - 1) // 3) + 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def mean_encode(df, feature, target):\n",
    "    ''' Mean-encode selected feature with target: \n",
    "        add mean tartget value for each level of the feature.\n",
    "        Then add them to the featues. '''\n",
    "    kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "    temp = [None] * df.shape[0]\n",
    "    temp = pd.Series(temp)\n",
    "\n",
    "    for idx_train, idx_val in kf.split(df):\n",
    "        train = df.iloc[idx_train]\n",
    "        validation = df.iloc[idx_val]\n",
    "\n",
    "        means = train.groupby(feature)[target].mean()\n",
    "        temp[idx_val] = validation[feature].map(means)\n",
    "\n",
    "    df[feature + '_mean_encoded'] = temp\n",
    "    mean_target = df[target].mean()\n",
    "    df[feature + '_mean_encoded'].fillna(mean_target, inplace=True)\n",
    "\n",
    "    # Correlation between mean_encoded feature and target\n",
    "    encoded_feature = df[feature + '_mean_encoded'].values\n",
    "    corr = np.corrcoef(df[target].values, encoded_feature)[0][1]\n",
    "    print('Correlation between mean_encoded {} and target ='.format(feature), corr)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_monthly(df, group_cols, agg_feature, agg_name, fill_na=0):\n",
    "    ''' Aggregate to get total monthly resuluts for the selected feature.'''\n",
    "    gb = df.groupby(group_cols, as_index=False)[\n",
    "        agg_feature].agg({agg_name: sum})\n",
    "    df_out = pd.merge(df, gb, how='left', on=group_cols).fillna(fill_na)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def add_lag_features(df, lag_cols, index_cols, max_lag):\n",
    "    ''' Add data from previous months (lags) for the selected features\n",
    "        to the list of features for training. '''\n",
    "    temp1 = df.copy()[index_cols + lag_cols]\n",
    "\n",
    "    for lag in np.arange(1, max_lag+1):\n",
    "        temp2 = temp1.copy()\n",
    "        temp2['date_block_num'] = temp2['date_block_num'] + lag\n",
    "        col_name = '_lag' + str(lag)\n",
    "        df = pd.merge(df, temp2, how='left', on=index_cols,\n",
    "                      suffixes=('', col_name)).fillna(0)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def lag_cor_plot(data, col, n_lags):\n",
    "    ''' Plot the changes and correlations of lagged-features (previous months) \n",
    "        with the current month values. '''\n",
    "    n_rows = n_lags//3  # 3 columns\n",
    "    plt.figure(figsize=(16, n_rows*4))  # hight=4 for each row\n",
    "    x = col\n",
    "    for i in range(1, n_lags+1):\n",
    "        y = x + '_lag' + str(i)\n",
    "        cor = np.corrcoef(data[x], data[y])[0, 1]\n",
    "\n",
    "        ax = plt.subplot(n_rows, 3, i)\n",
    "        ax.set(xticklabels=[])\n",
    "        ax.set(yticklabels=[])\n",
    "        ax.tick_params(axis=u'both', which=u'both', length=0)\n",
    "        ax.set_title('Correlation = %.2f' % cor)\n",
    "        sns.scatterplot(x, y, data=data, ax=ax)\n",
    "\n",
    "\n",
    "def get_train_test(df, target, cut_off):\n",
    "    ''' Train-test split: keep the last month for test. '''\n",
    "    max_months = max(df['date_block_num'])\n",
    "\n",
    "    # remove old data (before cut_off value)\n",
    "    train = df[df['date_block_num'].between(\n",
    "        cut_off, max_months-1)].drop('date_block_num', axis=1)\n",
    "    test = df[df['date_block_num'] ==\n",
    "                max_months].drop('date_block_num', axis=1)\n",
    "    \n",
    "    x_train = train.drop(target, axis=1)\n",
    "    y_train = train[target]\n",
    "\n",
    "    x_test = test.drop(target, axis=1)\n",
    "    y_test = test[target]\n",
    "\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales_train.csv')\n",
    "items = pd.read_csv('items.csv')\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops = pd.read_csv('shops.csv')\n",
    "\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to datetime format\n",
    "sales['date'] = pd.to_datetime(sales.date, format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the training data\n",
    "sales = pd.merge(sales, items, on='item_id',\n",
    "                 how='left').drop('item_name', axis=1)\n",
    "sales.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transactions with negative 'item_price' or 'item_cnt_day'\n",
    "price_negative = sales[sales['item_price'] <= 0]\n",
    "count_negative = sales[sales['item_cnt_day'] < 0]\n",
    "\n",
    "print(price_negative.shape, count_negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the only row with negative price\n",
    "# Items with negative count correspond to returns\n",
    "\n",
    "idx = price_negative.index[0]\n",
    "sales = sales.drop(idx, axis=0)\n",
    "\n",
    "# Get separate columns for sales and returns\n",
    "sales['isReturned'] = (sales['item_cnt_day'] < 0).astype(int)\n",
    "sales['return_cnt_day'] = abs(sales['item_cnt_day']) * sales['isReturned']\n",
    "sales['sell_cnt_day'] = abs(sales['item_cnt_day']) * (sales['item_cnt_day'] > 0).astype(int)\n",
    "sales = sales.drop('item_cnt_day', axis=1)\n",
    "\n",
    "get_info(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month, year, and season to features\n",
    "sales = add_date_features(sales)\n",
    "\n",
    "\n",
    "# Aggregate to get monthly data\n",
    "# 1. Total monthly sale for each item at each shop\n",
    "sales = aggregate_monthly(sales,\n",
    "                          ['date_block_num', 'shop_id', 'item_id'],\n",
    "                          'sell_cnt_day', 'monthly_sale', fill_na=0)\n",
    "# 2. Total monthly returns for each item at each shop\n",
    "sales = aggregate_monthly(sales,\n",
    "                          ['date_block_num', 'shop_id', 'item_id'],\n",
    "                          'return_cnt_day', 'monthly_return', fill_na=0)\n",
    "# 3. Total monthly sale for all items at each shop\n",
    "sales = aggregate_monthly(sales,\n",
    "                          ['shop_id', 'date_block_num'],\n",
    "                          'sell_cnt_day', 'monthly_sale_shop', fill_na=0)\n",
    "# 4. Total monthly sale for each item across all shops\n",
    "sales = aggregate_monthly(sales,\n",
    "                          ['item_id', 'date_block_num'],\n",
    "                          'sell_cnt_day', 'monthly_sale_item', fill_na=0)\n",
    "\n",
    "# Net monthly sales\n",
    "sales['total_monthly_sale'] = (\n",
    "    sales['monthly_sale'] - sales['monthly_return']).clip(lower=0)\n",
    "\n",
    "\n",
    "# Reorder columns, and sort\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "cols_other = list(sales.columns.difference(index_cols))\n",
    "temp = sales[index_cols + cols_other]\n",
    "sales = temp\n",
    "\n",
    "\n",
    "# Remove duplicate rows corresponding to daily data (different days with the \n",
    "# same monthly values). Keep the most recent row with the latest price.\n",
    "sales.sort_values(index_cols + ['date'], inplace=True)\n",
    "sales.drop_duplicates(subset=index_cols, keep='last', inplace=True) \n",
    "sales = sales.drop(['date', 'sell_cnt_day', 'return_cnt_day',\n",
    "                  'isReturned', 'monthly_sale'], axis=1)\n",
    "sales = sales.reset_index(drop=True)\n",
    "\n",
    "\n",
    "get_info(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag plots with 12-months lags of the total_monthly_sale\n",
    "\n",
    "data = sales.copy()\n",
    "\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "lag_cols = ['total_monthly_sale']\n",
    "data = add_lag_features(data, lag_cols, index_cols, 12)\n",
    "\n",
    "lag_cor_plot(data, 'total_monthly_sale', 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(data.corr(), xticklabels=data.columns,\n",
    "            yticklabels=data.columns, cmap=\"RdBu_r\");\n",
    "plt.title('Correlaiton between variables', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of features with target variable\n",
    "df = pd.DataFrame(data.corr().total_monthly_sale, index=data.corr().index)\n",
    "plt.figure(figsize=(4, 12))\n",
    "sns.heatmap(df, annot=True, cmap=\"RdBu_r\");\n",
    "plt.title('Correlaiton with target', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "Monthly sale for each item, at each shop = total sale for the previous month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BL = sales.copy()[['shop_id', 'item_id',\n",
    "                      'date_block_num', 'total_monthly_sale']]\n",
    "\n",
    "target = 'total_monthly_sale'\n",
    "# Train-test split\n",
    "x_train, y_train, x_test, y_test = get_train_test(df_BL, target, 12)\n",
    "\n",
    "\n",
    "train_BL = pd.concat([x_train, y_train], axis=1)\n",
    "train_BL = train_BL.drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\n",
    "\n",
    "test_BL = pd.concat([x_test, y_test], axis=1)\n",
    "test_BL = pd.merge(test_BL, train_BL, how='left', on=[\n",
    "    'shop_id', 'item_id']).dropna()\n",
    "\n",
    "\n",
    "rmse_BLmodel = mean_squared_error(test_BL['total_monthly_sale_x'],\n",
    "                                  test_BL['total_monthly_sale_y'], squared=False)\n",
    "print('The root mean squared error of prediction with baseline model is:',\n",
    "      rmse_BLmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales.copy()\n",
    "\n",
    "# Mean-encode 'item_id' and 'shop_id'\n",
    "target = 'total_monthly_sale'\n",
    "data = mean_encode(data, 'shop_id', target)\n",
    "data = mean_encode(data, 'item_id', target)\n",
    "\n",
    "\n",
    "# Add 6-months lags of total_monthly_sale, monthly_sale_shop, monthly_sale_item\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "lag_cols = ['total_monthly_sale', 'monthly_sale_shop', 'monthly_sale_item']\n",
    "data = add_lag_features(data, lag_cols, index_cols, max_lag=6)\n",
    "\n",
    "\n",
    "get_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split: Keep the last month for test\n",
    "\n",
    "target = 'total_monthly_sale'\n",
    "x_train, y_train, x_test, y_test = get_train_test(data, target, 12)\n",
    "\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model\n",
    "\n",
    "lr = make_pipeline(StandardScaler(), PCA(), LinearRegression())\n",
    "\n",
    "crossval_lr = cross_val_score(lr, x_train, y_train,\n",
    "                           cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "print('Root mean squared error with cross-validation:',\n",
    "      sqrt(-1.0 * crossval_lr.mean()))\n",
    "\n",
    "lr.fit(x_train, y_train)\n",
    "pred_lr = lr.predict(x_test)\n",
    "rmse_lr = mean_squared_error(y_test, pred_lr, squared=False)\n",
    "print('The root mean squared error of prediction with linear regression is:', rmse_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light gradient boosting algorithm\n",
    "\n",
    "lgbm = LGBMRegressor(feature_fraction=0.75,\n",
    "                     min_data_in_leaf=2**5,\n",
    "                     learning_rate=0.03,\n",
    "                     bagging_seed=2**7,\n",
    "                     num_leaves=2**7)\n",
    "\n",
    "crossval_lgbm = cross_val_score(lgbm, x_train, y_train,\n",
    "                                cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "print('Root mean squared error with cross-validation:',\n",
    "      sqrt(-1.0 * crossval_lgbm.mean()))\n",
    "\n",
    "lgbm.fit(x_train, y_train)\n",
    "pred_lgbm = lgbm.predict(x_test)\n",
    "rmse_lgbm = mean_squared_error(y_test, pred_lgbm, squared=False)\n",
    "print(('The root mean squared error of prediction with '\n",
    "      'light gradient boosting algorithm is:'), rmse_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Feature Importances\n",
    "importances = lgbm.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    importances, columns=['Feature Importance'], index=x_test.columns)\n",
    "\n",
    "feature_importance_df.sort_values(\n",
    "    by='Feature Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Feature importance plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.barplot(x='Feature Importance', y=feature_importance_df.index,\n",
    "            data=feature_importance_df, color=\"b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Future Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop('ID', axis=1)\n",
    "test_data.sort_values(by=['shop_id', 'item_id'], inplace=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "get_info(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features to test_data\n",
    "\n",
    "lookup = data.copy().drop_duplicates(\n",
    "    subset=['shop_id', 'item_id'], keep='last')\n",
    "lookup.drop(['date_block_num', 'total_monthly_sale'], axis=1, inplace=True)\n",
    "\n",
    "test = test_data.copy()\n",
    "test = pd.merge(test, lookup, how='left',\n",
    "                on=['shop_id', 'item_id']).fillna(0)\n",
    "\n",
    "test['month'] = 11\n",
    "test['year'] = 2015\n",
    "test['season'] = 4\n",
    "\n",
    "get_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lgbm.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open('model.txt', 'w') as file:\n",
    "    file.write(str(lgbm))\n",
    "\n",
    "\n",
    "np.savetxt('predictions.csv', predictions, delimiter=',')\n",
    "\n",
    "feature_importance_df.to_csv('feature_importance.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
